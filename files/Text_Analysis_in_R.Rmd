---
title: 'Text Analysis in R: online appendix'
author: "Kasper Welbers, Wouter van Atteveldt & Kenneth Benoit"
date: "2023"
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse = TRUE)
```

## About this document

This is the online appendix for [Welbers, van Atteveldt & Benoit (2017)](http://www.tandfonline.com/doi/full/10.1080/19312458.2017.1387238), that contains the example code presented in the article. The code in this appendix will be kept up-to-date with changes in the used packages, and as such can differ slightly from the code presented in the article.

<!-- In addition, this appendix contains references to other tutorials, that provide additional instructions for alternative, more in-dept or newly developed text anaylysis operations. -->

### required packages

The following packages have to be installed to run all the code examples. Note that the lines to install the packages only have to be run once.

```{r, eval=F}
################# PACKAGE       # SECTION IN ARTICLE
install.packages("readtext")    # data preparation
install.packages("stringi")     # data preparation

install.packages("quanteda")    # data preparation and analysis
install.packages('quanteda.textmodels') 
install.packages('quanteda.textstats')
install.packages('quanteda.textplots')

install.packages("topicmodels") # analysis

install.packages("spacyr")      # advanced topics
install.packages("corpustools") # advanced topics
```

## Data Preparation

### String Operations

```{r}
library(readtext)  
# url to Inaugural Address demo data that is provided by the readtext package 
filepath <- "https://raw.githubusercontent.com/kbenoit/readtext/master/inst/extdata/csv/inaugCorpus.csv"

rt <- readtext(filepath, text_field = "texts") 
rt
```

### String Operations

```{r}
library(stringi) 
x <- c("The first string", ' The <font size="6">second string</font>') 

x <- stri_replace_all(x, "", regex = "<.*?>")   # remove html tags 
x <- stri_trim(x)                               # strip surrounding whitespace
x <- stri_trans_tolower(x)                      # transform to lower case 
x
```

### Preprocessing

#### Tokenization

```{r}
library(quanteda) 

text <- "An example of preprocessing techniques" 
toks <- tokens(text)  # tokenize into unigrams 
toks
```

#### Normalization: lowercasing and stemming

```{r}
toks <- tokens_tolower(toks) 
toks <- tokens_wordstem(toks) 
toks
```

#### Removing stopwords

```{r}
sw <- stopwords("english")   # get character vector of stopwords 
head(sw)                     # show head (first 6) stopwords
tokens_remove(toks, sw)
```

### Document-Term Matrix

Since the publication of the Text Analysis in R paper, the quanteda package has gone through several updates. 
One important change is that many operations are now cut down into separate steps.
This works nicely together with the now common pipe notation (`|>`, or `%>%` in tidyverse).

Before, we created a dfm with one single do-it-all function. 
Now, we run our data through a pipeline of functions that each perform a single step.

```{r}
text <-  c(d1 = "An example of preprocessing techniques",  
           d2 = "An additional example",  
           d3 = "A third example") 

dtm <- text |>
  corpus() |>                          ## create quanteda corpus
  tokens() |>                          ## tokenize the corpus
  dfm() |>                             ## structure tokens as Document Term Matrix
  dfm_tolower() |>                     ## preprocessing: lowercase
  dfm_wordstem() |>                    ## preprocessing: stemming
  dfm_remove(stopwords('english'))     ## preprocessing: remove English stopwords
  
dtm
```

Create the DTM using the inaugural speeches (rt) that we read into R above.

```{r}
dtm <- rt |> 
  corpus() |> 
  tokens() |>
  dfm() |>
  dfm_tolower() |>
  dfm_wordstem() |>
  dfm_remove(stopwords('english')) 

dtm
```

### Filtering and weighting

```{r}
doc_freq <- docfreq(dtm)         # document frequency per term (column) 
dtm <- dtm[, doc_freq >= 2]      # select terms with doc_freq >= 2 
dtm <- dfm_tfidf(dtm)            # weight the features using tf-idf 
head(dtm)
```

## Analysis

Prepare DTM for analysis examples.

```{r}
dtm <- data_corpus_inaugural |>
  corpus() |> 
  tokens(remove_punct = T) |> 
  dfm() |>
  dfm_tolower() |> 
  dfm_wordstem() |>
  dfm_remove(stopwords('english'))

dtm
```

### Counting and Dictionary

```{r}
myDict <- dictionary(list(terror = c("terror*"), 
                          economy = c("job*", "business*", "econom*"))) 
dict_dtm <- dfm_lookup(dtm, myDict, nomatch = "_unmatched") 
tail(dict_dtm)
```

### Supervised Machine Learning

```{r}
library(quanteda)
library(quanteda.textmodels)
```

```{r}
set.seed(2) 
# create a document variable indicating pre or post war 
docvars(dtm, "is_prewar") <- docvars(dtm, "Year") < 1945 

# sample 40 documents for the training set and use remaining (18) for testing 
train_dtm <- dfm_sample(dtm, size = 40)
test_dtm <- dtm[setdiff(docnames(dtm), docnames(train_dtm)), ] 

# fit a Naive Bayes multinomial model and use it to predict the test data 
nb_model <- textmodel_nb(train_dtm, y = docvars(train_dtm, "is_prewar")) 
pred_nb <- predict(nb_model, newdata = test_dtm)

# compare prediction (rows) and actual is_prewar value (columns) in a table 
table(prediction = pred_nb, is_prewar = docvars(test_dtm, "is_prewar"))
```

### Unsupervised Machine Learning

```{r}
library(topicmodels) 

texts = corpus_reshape(data_corpus_inaugural, to = "paragraphs")

par_dtm <- texts |> corpus() |> tokens(remove_punct = T) |> 
  dfm() |> dfm_tolower() |> dfm_wordstem() |> 
  dfm_remove(stopwords('english')) |> dfm_trim(min_count = 5) |>
  convert(to = 'topicmodels')

set.seed(1)
lda_model <- topicmodels::LDA(par_dtm, method = "Gibbs", k = 5) 
terms(lda_model, 5)
```

### Statistics

```{r}
library(quanteda.textstats)
library(quanteda.textplots)

# create DTM that contains Trump and Obama speeches
dtm_pres <- data_corpus_inaugural |>
  corpus_subset(President %in% c('Obama','Trump')) |>
  tokens(remove_punct = T) |> 
  dfm() |>
  dfm_remove(stopwords('english'))

# compare target (in this case Trump) to rest of DTM (in this case only Obama).
dtm_pres |>
  dfm_group(President) |>
  textstat_keyness(target = "Trump") |>
  textplot_keyness()
```


## Advanced Topics

### Advanced NLP

```{r, eval=F}
library(spacyr) 
spacy_install()
spacy_initialize()
d <- spacy_parse("Bob Smith gave Alice his login information.", dependency = TRUE) 
d[, -c(1,2)]
```

### Word Positions and Syntax

```{r}
text <- "an example of preprocessing techniques" 

text |>
  tokens() |>
  tokens_ngrams(n=3, skip=0:1)
```

```{r}
library(corpustools)
 
tc <- create_tcorpus(sotu_texts, doc_column = "id") 
hits <- search_features(tc, '"freedom americ*"~5')
kwic <- get_kwic(tc, hits, ntokens = 3) 
head(kwic$kwic, 3)
```
